\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage[utf8]{inputenc}
\usepackage{gensymb}
\usepackage{graphicx}
\graphicspath{ {imgs/} }
\usepackage{float}

\cvprfinalcopy
\def\cvprPaperID{a1700831}


% begin of document
\begin{document}
\title{Assignment 1 - Method for Creating Mosaics by Brown and Lowe}
\author{Yuanzhong Xia\\
University of Adelaide\\
SA, Australia\\
{\tt\small a1700831@student.adelaide.edu.au}
}
\maketitle

% abstract
\begin{abstract}
This is the assignment 1 report, describing and testing a method for creating panorama from a set of smaller images.
The related codes are using Java binding version of OpenCV, and it contains my partial implemented improved method, which is used to test my hypothesis and improvements.
\end{abstract}


\section{The Problem}
The problem is to stitch many small images representing for different part of the area space into a large panorama image.
However, the small images often have lots of issues when stitching them together.
The images can have different rotations, scales, affine, projective transformations, photometric problems, moving objects and so on.

To stitch the images into a large panorama, the method must overcome most of the issues above,
and the most important thing is to make sure the image look normal and edges are less noticeable.


\section{Brown and Lowe's Method}
The method is being discovered for a long time. But none of them can perform better than Brown and Lowe's method \cite{origin}.
Brown and Lowe's method can overcome the issues caused by variant orientations, zoomings, and do color corrections and moving object detection to some extend.

The method is based on invariant features appearing in multiple images. If the combination of invariant features in two images match with each other,
they can be stitched together by superposing the matching features, and vise versa, they cannot stitch images if they don't have invariant features. 

\subsection{Description}
The psedocodes on the paper have already show the outline the stitching algorithm.
Here are the more detailed description:
\begin{enumerate}
    \item
        Read all input images,
        \textit{(if the number of images is less than two, \textbf{fail} because one image does not need to be stitched.)}
    \item \label{n:e}
        Extract SIFT \cite{sift} features from all images,
    \item \label{n:k}
        Store $k$-nearest neighbor features using a k-d tree \cite{knn},
        \textit{(increase the accuracy for feature matching.)}
    \item \label{n:m}
        Match the potential images using RANSAC \cite{ransac}, and verify them using a probabilistic model;
        a feature of this method is the images can be divided as disjoint set which means small images from differnt scenes can be distinguished and stitched separately,
        \textit{(if the probability is not high enough, and there isn't any image can be stitched with this image, \textbf{fail}.)}
    \item \label{n:r}
        Reshape and align the images using bundle adjustment \cite{bm},
        \textit{(make the stitching seamless.)}
    \item \label{n:s}
        Straighten the whole panorama by silumating the centric camera and projection effect,
        \textit{(eliminate issues caused by different camera orientation.)}
    \item \label{n:g}
        Gain compensation by weighted averaging the pixels of overlapped areas,
        \textit{(remove significant stitching edges.)}
    \item \label{n:a}
        Apply multi-band blending to the final images,
        \textit{(removed overlapping effect produced by gain compensation, caused by inaccurate match.)}
    \item \label{n:o}
        Output the panorama image.
\end{enumerate}

% idea of simulating a wider view filed len
% todo: add some numeric analysis
% todo: more on probabilistic model

\subsection{Limitation and Improvements}
In the Section 9 of the original paper, Brown and Lowe put some potential future works which are the weaknesses and possible improvements for current version of method.
\begin{itemize}
    \item
        Camera position is often moved (at slightly) when trying to take lots of photos in real life.
        That means the image aligning cannot be finished by simple reshaping because of scene depth and shading.
        Usually, the resulting image can contain overlapping effects.

        Some of them can be deblurred by Bascle \textit{et al.}'s method \cite{bascle}.
        The rest of them might require 3D modeling for perfect fix.
    \item
        Scene can also be dynamical. Moving objects like trees and birds can also affect the stitching result.
        Although multi-band blending can solve some kinds of blur effect caused by slight motion,
        it doesn't work for large motion case.

        The way to solve it might be detecting the same object, and keeping one complete copy or removing the incomplete object.
    \item
        Advanced camera modeling. This is what I first came out when I saw the title with ``using invariant feature''.
        The method is limited into 2D space. In 3D space, objects might obstruct the view, and when the camera moves, the objects will distort a lot.
        Just like Fig \ref{fig:cameramotion}, the camera orientation is vertical to the horizon line on the wall, and the motion direction is parallel with the wall.
        It is very clear that the table and chair on the images distort heavily.
        
        With 3D modeling or simulating a scanner, the issue can be solved.
    \item
        Color correction is also not perfect.
        Influenced by gain compensation, the overlapped area is evaluated by error rate first,
        then the gain parameters for the whole images are calculated by a quadratic objective function.
        However, the resulting image can be easily found to have abnormal colors in overlapped areas (e.g. the overlapped area is yellowing).

        One way to solve the problem mentioned in the paper is to uniforming the colors from a common source first, like sky.
        In this way, some issues caused by over exposure images can also be fixed.
\end{itemize}

\begin{figure*}
    \begin{center}
        \includegraphics[width=0.9\textwidth]{horizontal_motion}
    \end{center}
    \caption{The source images are captured by horizontal moving camera, so the space projection structure is changed in every image,
    and it is impossible to match features using mentioned image matching algorithm.}
    \label{fig:cameramotion}
\end{figure*}

The weaknesses above are still cucrrent research topics, which are not easy to solve by myself.

I have two patching extensions.

The \textbf{first} one is a more specific panorama stitching case - scanning image by moving camera.
This problem is different from the first mentioned weakness above.
The images to be stitched are in the same plane, so there are no view depth and no shading effect.
However, no matter how the photos are taken, by a centric camera or by a moving camera, the resulting images cannot look like a plane.
i.e. For local photos of a whole graph, the resulting image is expected to be like a scanned graph.

To solve this problem, the algorithm should accept another parameter ``plane mode'',
after applying Brown and Lowe's method, the algorithm should find the rectangle outlines of the plane area
(in the resulting, it is often a curved quadrangle. (see the yellow lines in Fig \ref{fig:cml}.)
Then, reshape the final image to make the outline be a rectangle.

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=0.9\linewidth]{line_combine}
    \end{center}
    \caption{After the stitching method, straight lines are often reshaped into curves.}
    \label{fig:cml}
\end{figure}

The \textbf{second} extension is when the image doesn't comtain enought features.
Using the OpenCV implementation, if the images are very simple (like Fig. \ref{fig:pm}),
the probabilistic model will tell that the images don't have enough probabilities to match correctly.
The they cannot be stitched.

The optimization patching solution is to reduce the $p_{min}$ in Fomula (12) in the original paper when there are not potential matchable images.

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=0.9\linewidth]{pure_color}
    \end{center}
    \caption{Images with pure colors.}
    \label{fig:pc}
\end{figure}

% blur effect

Brown and Lowe's method uses the feature that the images are assumed to be captured by a centric camera,
because in the ``straightening'' method, they ignore the case that the camera is not moved horizontally.
They use a camera position detection idea to straighten the long curved image into long straight image.
Therefore, if the images are taken horizontally, they will be unable to be stitched together in that no two images can match by features.

% improvements


\section{Testing}
% non-free SIFT
\textit{(the OpenCV implementation uses speeded up robust features (SURF) \cite{surf} to generate the feature)}



At the beginning, I tried to experiment in a measurable way, which can produce some mathematical benchmark results. Here are the steps:
\begin{enumerate}
    \item Select a long image, \textit{(produce the images from panorama.)}
    \item Draw a horizontal line with 2 pixel width from left most to right most in vertical center, \textit{(create a baseline for benchmark, but the line should not affect the extraccted feature set of the original images.)}
    \item Split the image into three images: left, middle and right (each adjacent two images have 30\% shared area, and have same width), \textit{(make it simple, use only three images.)}
    \item Distort the left image to make the length of its left edge is two times larger than the length of its right edge, \textit{(simulate the projection effect.)}
    \item Distort the right image to make the length of its right edge is two times large than the length of its left edge, \textit{(simulate the projection effect.)}
    \item Rotate the left image for 30\degree clockwise, \textit{(just a random value to test , because idealy, the feature matching algorithm can match SIFT feature in any orientation.)}
    \item Zoom out to fit the rectangle window, \textit{(the image should still be a rectangle.)}
    \item Stitch them with Brown and Lowe's methods, \textit{(do stitching.)}
    \item Measure the following values, \textit{(metric test.)} 
    \begin{itemize}
        \item The average running time with 30 repeatitions in millisecond,
        \item The mean square error (MSE) of the baseline's curvity,
        \item The minimum distance between each two baselines' endpoints in pixel, larger than 0 if the three baseline segments (each image has one) are not stitched together,
    \end{itemize}
\end{enumerate}

% but

\begin{figure}[t]
    \begin{center}
        \includegraphics[width=0.9\linewidth]{fail}
    \end{center}
    \caption{The source image is a screenshot (plane), and the 3 images for stitching are cropped from the source image.
    Even when I tried using images with more than 70\% shared area, the result is still out of expectation.}
    \label{fig:fail}
\end{figure}



Hypothesis and testing, and resulting.

Anaylse the outcome of the tests, and give a useful conclusion.

\subsection{}


% Bibliography
\begin{thebibliography}{9}
\bibitem {origin}
M. Brown and D. Lowe, ``Automatic panoramic image stitching using invariant features,''
\textit{Int. J. Comput. Vision}, vol. 74, no. 1, pp. 59–73, 2007.

\bibitem {sift}
D. G. Lowe, ``Distinctive Image Features from Scale-Invariant Keypoints,''
\textit{International Journal of Computer Vision}, vol. 60, 2004.

\bibitem {knn}
Beis, J. and Lowe, D. 1997. ``Shape indexing using approximate nearest neighbor search in high-dimensional spaces,''
\textit{In Proceedings of the Interational Conference on Computer Vision and Pattern Recognition (CVPR97)}, pp. 1000–1006.

\bibitem {ransac}
Fischler, M. and Bolles, R. 1981. Random sample consensus: ``A paradigm for model fitting with application to image analysis and automated cartography'',
\textit{Communications of the ACM}, 24:381–395.

\bibitem {bm}
Triggs, W., McLauchlan, P., Hartley, R., and Fitzgibbon, A. 1999. ``Bundle adjustment: A modern synthesis,''
\textit{In Vision Algorithms: Theory and Practice}, number 1883 in LNCS. Springer-Verlag. Corfu, Greece, pp. 298–373.

\bibitem {bascle}
Bascle, B., Blake, A., and Zisserman, A. 1996. ``Motion deblurring and super-resolution from and image sequence,''
\textit{In Proceedings of the 4th European Conference on Computer Vision (ECCV96)}. Springer-Verlag, pp. 312–320.

\bibitem {surf}
H., Bay, T., Tuytelaars and L., Van Gool, ``Surf: Speeded up robust features,''
\textit{Lecture Notes in Computer Science}, Vol.3951, p.404, 2006.

\end{thebibliography}

\end{document}

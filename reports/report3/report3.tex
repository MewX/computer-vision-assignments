\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage[utf8]{inputenc}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathtools}
\graphicspath{ {report3-imgs/} }
\usepackage{float}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

\cvprfinalcopy
\def\cvprPaperID{a1700831}


% begin of document
\begin{document}
\title{Assignment 3 - Method for Generating Image Descriptions}
\author{Yuanzhong Xia\\
University of Adelaide\\
SA, Australia\\
{\tt\small a1700831@student.adelaide.edu.au}
}
\maketitle

% abstract
\begin{abstract}
% TODO: abstract
\end{abstract}

% content
\section{The Problem}
The problem is to give a sentence in natural language of the description for an input image.

This problem is very typical, because it is very challenging that it transforms information from ``image'' type to ``text'' type.
The input image should firstly be understood by the artificial intelligence,
then it should be able to output a descriptive text to tell what the input image shows.

The relative applications can be various, like: helping human to automatically pointing out the objectives and events in a massive number of images,
and helping visually impaired people to see. But the most difficult challenge is not only understanding both image and natural language,
but also the information transformation.


\section{Background}
Before this paper by Karpathy \textit{et al.} \cite{origin}. There are plenty of researches challenging this problem,
which can be mainly divided into three types.

\subsection{Image annotating strategy}
A typical way to annotate images is classifying or tagging images \cite{everingham}, \cite{russakovsky}.
In a fixed word dictionary, the image tagging can be more meaningful because unwanted words are discarded at the beginning.
The weakness is that the words in dictionary are often not rich enough.

Barnard \textit{et al.} \cite{barnard} and Socher \textit{et al.} \cite{socher}'s researches uses the images,
whose segments are annotated instead the whole image, as the training input data.
That helps the object recognition process, but requires more human works.

Gould \textit{et al.}'s research \cite {gould} used a method to extract scene feature,
so that the relationships between objects and the scene can be easily connected.
However, their work was focusing on labelling the scene, while the proposed method is used to
generate richer and high-level descriptions of the image, no matter what the scene labels are.


\subsection{Description generating strategy}
Methods from Kulkarni \textit{et al.} \cite{kulkarni} limit the resulting sentence number in one;
whereas this paper's authors think that manual restriction is not necessary and it reduces the artificial intelligence's creativity.

Matuszek \textit{et al.} \cite{matuszek} used grounding dependency tree relations which arrange words in a vector space.
The vector space would limit the results to some extend, like the sentence length, etc.


\subsection{Connecting between images and natural language strategy}
The earlier pioneering methods by Farhadi \textit{et al.} \cite{farhadi} hard-coded visual concepts and explicitly defined sentence.
Images objects or scenes are pre-defined, then marked images are learnt by the system so that the system can recognize defined things.
However, these methods limited the language variety and required much human works to define the connecting between language words and the images first.

Now, pre-trained image/word vectors are quite popular.
For images, ImageNet \cite{imagenet} is a very popular pre-trained image recognition model;
for words, previous works \cite{bengio}, \cite{socher2}, \cite{mikolov} mentioned the pre-trained word vectors
to obtain low-dimensional representations of words.
Additionally, in the original paper, the authors use these pre-trained models as well.


\section{Algorithm Description}
Unlike the previous approaches, the proposed method in Karpathy \textit{et al.}'s paper is to let the artificial neural networks
learn the pattern between images and sentences automatically.
Therefore there is no limitation for this method literally,
because the training data can be arbitrary, and the model thus will be adjusted by the training data.

The overview of training algorithm steps is as follows,
\begin{enumerate}
    \item Prepare the training images and corresponding sentences;
          \textit{(datasets mentioned in the paper are flickr8k, flickr30k and MSCOCO,
          and sometimes each image has multiple descriptive sentences.)}
    \item Extract features using pre-trained CNN model on ImageNet \cite{imagenet};
          \textit{(this model contains objects of 200 classes, which is described in ImageNet Detection Challenge \cite{inch}.)}
    \item Calculate the image representation matrix using top 19 detected known object by ImageNet CNN plus the whole image matrix;
          \textit{(the representation matrix in the paper is defined in $h \times 4096$ dimensions, thus this is the recognizable resolution.)}
    \item Train the sentence generator multimodal RNN is trained to take a word to predict the next following word, by passing the word sequence and the image representation matrix;
          \textit{(``START'' and ``END'' are two special tokens in this RNN, and the words are sent to the RNN one by one.)}
\end{enumerate}
% TODO: how to tag more effectively (+5 times selected)

To test an image:
\begin{enumerate}
    \item Prepare testing images;
    \item Extract features using pre-trained CNN model on ImageNet;
          \textit{(same as training.)}
    \item Calculate the image representation matrix using top 19 detected known object by ImageNet CNN plus the whole image matrix;
          \textit{(same as training.)}
    \item Generate description from multimodal RNN by setting ``START'' token and image representation matrix;
          \textit{(until ``END'' is generated, the description is finished generating.)}
\end{enumerate}

Besides, the original paper also mentions the methods for predicting region text information, to mark the regions like Fig. \ref{fig:region}:
\begin{enumerate}
    \item When having the image representation matrix;
    \item Calculate the sentence representation vector using Bidirectional Recurrent Neural Network (BRNN) \cite{brnn};
          \textit{(each input sentence sequence is transformed into a $h$ dimensional vector;
          during the calculation, BRNN takes two directional word relations to maintain a context;
          then, a word vocabulary is built automatically by the methods from Mikolov et al. \cite{mikolov}.)}
    \item Calculate image-sentence score by dot product between sentence representation vector and image representation matrix and a $max$ operation;
          \textit{(this method is proved by their previous work \cite{karpathy},
          and the resulting image-sentence score is actually calculated by the image region-word score.)}
    \item Align words with corresponding regions in Markov Random Field (MRF) to get an annotated region set;
          \textit{(each region has multiple words aligned with;
          to find the best alignment, the authors use dynamic programming by Viterbi et al. \cite{viterbi}.)}
\end{enumerate}

\begin{figure*}
    \begin{center}
        \includegraphics[width=0.9\textwidth]{region.png}
    \end{center}
    \caption{The region prediction from the original paper. This figure is the Fig. 9 in original paper.}
    \label{fig:region}
\end{figure*}


\section{Hypothesis}
% 1. two data model problem: one object in one model, not in another model (PS an object)
% 2. input generation size not enough, use small object with high resolution


\section{Experiments}
% details set up


\section{Limitations}


\section{Conclusion}


% Bibliography
\begin{thebibliography}{99}
\bibitem {origin}
A. Karpathy and L. Fei-Fei, Deep visual-semantic alignments for generating image descriptions,
In \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp. 3128-3137, 2015.

\bibitem {kulkarni}
G. Kulkarni, et al., Baby talk: Understanding and generating simple image descriptions,
In \textit{Proc. IEEE Conf. Comput. Vis. Pattern Recog.}, pp. 1601-1608, 2011.

\bibitem {farhadi}
A. Farhadi, et al., Every picture tells a story: Generating sentences from images,
In \textit{Proc. 11th Eur. Conf. Comput. Vis.}, pp. 15-29, 2010.

\bibitem {everingham}
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, The Pascal visual object classes (VOC) challenge,
In \textit{Int. J. Comput. Vis.}, vol. 88, no. 2, pp. 303–338, 2010.

\bibitem {russakovsky}
O. Russakovsky, et al., Imagenet large scale visual recognition challenge,
In \textit{Int. J. Comput. Vis.}, vol. 115, no. 3, pp. 211–252, 2015.

\bibitem {barnard}
K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D. M. Blei, and M. I. Jordan, Matching words and pictures,
In \textit{J. Mach. Learn. Res.}, vol. 3, pp. 1107–1135, 2003.

\bibitem {socher}
R. Socher and L. Fei-Fei, Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora,
In \textit{Proc. IEEE Conf. Comput. Vis. Pattern Recog.}, pp. 966–973, 2010.

\bibitem {gould}
S. Gould, R. Fulton, and D. Koller, Decomposing a scene into geometric and semantically consistent regions,
In \textit{Proc. IEEE 12th Int. Conf. Comput. Vis.}, pp. 1–8, 2009.

\bibitem {matuszek}
C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox, A joint model of language and perception for grounded attribute learning,
In \textit{Proc. 29th Int. Conf. Mach. Learn.}, pp. 1671–1678, 2012.

\bibitem {bengio}
Y. Bengio, H. Schwenk, J.-S. Senecal, F. Morin, and J.-L. Gauvain, Neural probabilistic language models,
In \textit{Innovations in Machine Learning. Berlin, Germany}, 2006.

\bibitem {socher2}
R. Socher, J. Pennington, and C. Manning, Glove: Global vectors for word representation,
In \textit{Proc. Empirical Methods Natural Language Process.}, pp. 1532–1543, 2014.

\bibitem {mikolov}
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, Distributed representations of words and phrases and their compositionality,
In \textit{Proc. Advances Neural Inf. Process. Syst.}, pp. 3111–3119, 2013.

\bibitem {imagenet}
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ImageNet: A large-scale hierarchical image database,
In \textit{Proc. IEEE Conf. Comput. Vis. Pattern Recog.}, pp. 248–255, 2009.

\bibitem {inch}
O. Russakovsky, et al., Imagenet large scale visual recognition challenge,
In \textit{Int. J. Comput. Vis.}, vol. 115, no. 3, pp. 211–252, 2015.

\bibitem {brnn}
M. Schuster and K. K. Paliwal, Bidirectional recurrent neural networks,
In \textit{IEEE Trans. Signal Process.}, vol. 45, no. 11, pp. 2673–2681, 1997.

\bibitem {karpathy}
A. Karpathy, A. Joulin, and F. F. F. Li, Deep fragment embeddings for bidirectional image sentence mapping,
In \textit{Proc. Advances Neural Inf. Process. Syst.}, pp. 1889–1897, 2014.

\bibitem {viterbi}
A. J. Viterbi, Error bounds for convolutional codes and an asymptotically optimum decoding algorithm,
in \textit{IEEE Trans. Inf. Theory}, vol. TIT-13, no. 2, pp. 260–269, 1967.

\end{thebibliography}

\end{document}
